{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Building an Artificial Neural Network\n",
    "    Layers:Dense, Convolutional, Pooling, Recurrent, Normalization, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "input_shape defines the input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(5, input_shape=(3,), activation=\"relu\"),\n",
    "    Dense(2, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Activation Functions:\n",
    "Sigmoid -> [0,1]\n",
    "Relu -> max(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(5, input_shape=(3,)))\n",
    "model.add(Activation(\"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training and Learning\n",
    "Solving Optimization of Weights -> with Stochastic Gradient Descent(SGD)\n",
    "Minimizing loss function, or the error of the output\n",
    "gradient of d(loss)/d(weight) and multiply by learning rate which is between .01 and .0001 to update given weights\n",
    "each change of weights and loss function happens at each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(16, input_shape=(2,), activation=\"relu\"),\n",
    "    Dense(32, activation=\"relu\"), \n",
    "    Dense(2, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adam is an SGD\n",
    "#Compiling Model\n",
    "model.compile(Adam(lr=.001), loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-67af83328061>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-12-67af83328061>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    model.fit(scaled_train_samples, train_labels, validation_split=.2 batch_size=10, epochs=20, shuffle=True, verbose=2)\u001b[0m\n\u001b[1;37m                                                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Training model\n",
    "#first two params are two numpy arrays holding training data\n",
    "#batch size is how many pieces of data to be sent to model at once\n",
    "#verbose is how much output we want to see\n",
    "model.fit(scaled_train_samples, train_labels, validation_split=.2 batch_size=10, epochs=20, shuffle=True, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a validation set as validation data with validation_split=0.2 parameter as above \n",
    "                                        OR\n",
    "We can use actual validation data in the form of list of tuples valid_set = [(sample, label), (sample, label), ...]\n",
    "and use validation_data=valid_set parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Loss Function\n",
    "Using Mean Square Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sparse_categorical_crossentropy'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Specifying loss func\n",
    "model.loss = 'sparse_categorical_crossentropy'\n",
    "model.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Learning Rate\n",
    "gradient of d(loss)/d(weight) and multiply by learning rate which is between .01 and .0001 to update given weights by subtracting old weight by the gradient multiple value\n",
    "High learning rate (.01) risks overshooting and shooting past the minimum optimization of the loss function\n",
    "Should use lowr learning rate (at lowest .0001)\n",
    "\n",
    "Used Adam(lr=.0001) optimizer previously in model compile, lr is optional to default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'learning_rate:0' shape=() dtype=float32, numpy=0.01>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Specifying learning rate\n",
    "model.optimizer.lr = .01\n",
    "model.optimizer.lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prediction and Test Sets\n",
    "Test set should have samples and be unlabeled\n",
    "scaled_test_sample = where our test data samples held"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(scaled_test_samples, batch_size=10, verbose=0)\n",
    "for i in predictions:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Overfitting and Underfitting\n",
    "Overfitting: model gets really good predicting training data but not untrained data\n",
    "    If validation or test accuracy is bad and training is good then overfitting is occurring\n",
    "    Add more data to get rid of this issue\n",
    "    Data Augmentation, which adds and modifies data, can help with this too\n",
    "    Reducing complexity, such as neurons and layers, can help with this too\n",
    "    Increase dropout, which is dropping some neurons, can help too\n",
    "Underfitting: model cannot classify even training data let alone new test data\n",
    "    If training data scores are poor then it is underfit\n",
    "    To reduce, can increase complexity of model(ie. adding neurons and layers or type of layers)\n",
    "    Can add more features to input samples(such as adding opening price, volume, etc. for stock prediction algorithm)\n",
    "    Reduce Dropout for purposes of training, and if validation data is doing better than training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Supervised Unsupervised Learning\n",
    "samples are labeled, and machine creates mapping between these two\n",
    "during training, images and samples are supplied with labels, to classify image and find error of its findings to learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#weight, height\n",
    "train_samples = [(150,67), (130,68), (200,65), (125,52), (230,70), (181, 70)]\n",
    "train_samples = np.array(train_samples)\n",
    "\n",
    "# 0: male\n",
    "# 1: female\n",
    "train_labels = [1,1,0,1,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training model\n",
    "model.fit(x=train_samples, y=train_labels, batch_size=3, epochs=10, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Semi-Supervised Learning\n",
    "If we have lots of data and only some is labeled, we can use pseudo labeling\n",
    "\n",
    "Pseudo labeling is training the NN on the labeled data using supervised learning, then we predict the labels of the unlabeled set of data and label it based on the predictions.\n",
    "Now we train model on the complete pseudo labeled data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Unsupervised Learning\n",
    "Sample data is unlabeled\n",
    "Ex. Clustering Algorithms, AutoEncoders(outputs a reconstruction of input to artificial NN)\n",
    "Autoencoder can reconstruct image and helps with denoising stuff\n",
    "Unsupervised Learning maps samples to spaces to make decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Data Augmentation\n",
    "Creating new data off modifications to existing data in training set\n",
    "Ex. Cropping, flipping images, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. One Hot Encoding\n",
    "Labels are passed as integers (One Hot Encoding) and not words\n",
    "Encodes into vectors of 0s and 1s\n",
    "One Hot Encoding puts vector[x,x,x,...] given how many classifications there are: this one shows 3 categories [x,x,x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Convolution Neural Networks\n",
    "Mainly used for image analysis and pattern detection. \n",
    "It has several convolutional layers, and its filters detect patterns, such as objects, edges, and features. \n",
    "More sophisticated objects as it gets deeper. \n",
    "## Filters\n",
    "Each convolutional layer has a filter matrix of size nxm\n",
    "    Convolutional filters map by using the inner product of the image matrix\n",
    "    Output image size is smaller than input image size after convolving with filter, since edges are \"removed\"\n",
    "nxn image\n",
    "fxf filter\n",
    "output size = (n-f+1)x(n-f+1)\n",
    "## Zero Padding\n",
    "We add a border of pixels with value 0 around the borders of our image to combat losing essential border info\n",
    "    \"valid\" - no padding\n",
    "    \"same\" - padding to make output size same as input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 20, 20, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 18, 18, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 64)        51264     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 8, 128)         401536    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 16386     \n",
      "=================================================================\n",
      "Total params: 473,890\n",
      "Trainable params: 473,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#kernel_size is the filter size\n",
    "#valid padding causes output size to decrease\n",
    "model_valid = Sequential([\n",
    "    Dense(16, input_shape=(20,20,3), activation='relu'),\n",
    "    Conv2D(32, kernel_size=(3,3), activation='relu', padding='valid'),\n",
    "    Conv2D(64, kernel_size=(5,5), activation='relu', padding='valid'),\n",
    "    Conv2D(128, kernel_size=(7,7), activation='relu', padding='valid'),\n",
    "    Flatten(),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model_valid.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 20, 20, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 20, 20, 32)        4640      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 20, 20, 64)        51264     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 20, 20, 128)       401536    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 51200)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 102402    \n",
      "=================================================================\n",
      "Total params: 559,906\n",
      "Trainable params: 559,906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#same padding causes output size to be the same as shown in summary\n",
    "model_valid = Sequential([\n",
    "    Dense(16, input_shape=(20,20,3), activation='relu'),\n",
    "    Conv2D(32, kernel_size=(3,3), activation='relu', padding='same'),\n",
    "    Conv2D(64, kernel_size=(5,5), activation='relu', padding='same'),\n",
    "    Conv2D(128, kernel_size=(7,7), activation='relu', padding='same'),\n",
    "    Flatten(),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model_valid.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Max Pooling\n",
    "Operation typically added to CNN following convolution layers\n",
    "<br>\n",
    "Reduces dimensionality of output after convolving, possibly helps overfitting\n",
    "<br>\n",
    "filter size = 2x2\n",
    "<br>\n",
    "stride = 2\n",
    "<br>\n",
    "store max value within filter size of output image as a pool and store it in a new matrix forming a lower dimension image\n",
    "dimension is lowered by a factor of the stride value\n",
    "<br>\n",
    "There is also average pooling, where we take the average from the pool; however, max pooling is used the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.layers.convolutional import *\n",
    "from keras.layers.pooling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 20, 20, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 20, 20, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 10, 10, 32)        0         \n",
      "=================================================================\n",
      "Total params: 4,704\n",
      "Trainable params: 4,704\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#must create MaxPooling2D layer since Conv layers are 2D\n",
    "#pool_size is the pool matrix size\n",
    "#strides are the number of strides to move pool each time\n",
    "#don't use padding on maxpooling layers\n",
    "model = Sequential([\n",
    "    Dense(16, input_shape=(20,20,3), activation='relu'),\n",
    "    Conv2D(32, kernel_size=(3,3), activation='relu', padding='same'),\n",
    "    MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid')\n",
    "    Conv2D(64, kernel_size=(5,5), activation='relu', padding='same'),\n",
    "    Flatten(),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Backpropagation\n",
    "SGD updates the weights to optimize the loss function, through the process of backpropagation, where we calculate the gradient of the loss wr to the the weight.\n",
    "<br>\n",
    "Forward propagation is multiplying an output by the weight and moving it through the next layer's activation function.\n",
    "<br>\n",
    "Backpropagation is updating the weights to minimize the loss function by calculating the gradient through SGD to update the weights. Thus, gradient descent occurs via backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Vanishing Gradient\n",
    "The gradients calculated during backprop occurring in the earlier parts of the network have very small values and when updating the weights may not have a significant enough change to effectively minimize the loss\n",
    "<br>\n",
    "The early weights effected by a vanishing gradient causes the rest of the network to not learn as effectively as well\n",
    "\n",
    "# 16. Exploding Gradient\n",
    "The gradient calculated during backprop occuring in the earlier parts may have many large values multiplied, creating an exploding gradient. This is an unstable gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17. Weight Initialization\n",
    "When we compile our NN model, the weights are randomly selected with a normal distribution of mean 0, std 1\n",
    "<br>\n",
    "However, this caan cause vanishing/exploding gradient and unstable learning\n",
    "<br>\n",
    "To solve this, we can change the var(weights) from 1 to 1/n. \n",
    "<br>\n",
    "Xavier Initialization: compile weights randomly with normal dist first. Then multiply weights * sqrt(1/n).\n",
    "Can also use 1/(nin + nout) but fine to use 1/n instead\n",
    "<br>\n",
    "If we use relu as our activation function, use var(weights) = 2/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kernel_initializer sets weight initialization\n",
    "#glorot_uniform is the Xavier initialization(1/n) : this is Default\n",
    "#glorot_normal is normal distribution\n",
    "model = Sequential([\n",
    "    Dense(16, input_shape=(1,5), activation='relu'),\n",
    "    Dense(32, activation='relu', kernel_initializer='glorot_uniform'),\n",
    "    Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18. Bias\n",
    "Each neuron has a bias that learns (updates)\n",
    "<br>\n",
    "Bias increases the flexibiity of the model \n",
    "<br>\n",
    "Bias determines if a neuron is activated\n",
    "<br>\n",
    "We don't need to play with them too much since biases are learnable by the network as we learn from the data just like weights\n",
    "##### When using Relu\n",
    "any output at zero or below does not fire, any value higher is firing; however, the bias here is assumed to be 0\n",
    "<br>\n",
    "If we want our threshold to move from 0 to -1, then we make our bias 1. The bias is essentially the opposite of what we want to move our threshold to. This is because the activation function is (a1w1 + a2w2 + ... + b), where b is the bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19. Learnable/Trainable Parameters for a Standard NN\n",
    "learnable parameters are all of the weights and biases of the network\n",
    "<br>\n",
    "learnable parameters for a single layer = (weights x outputs) + biases\n",
    "<br>\n",
    "Biases are usually the same amount as the output of that layer\n",
    "<br>\n",
    "sum this for all layers to find learnable parameters for the whole network\n",
    "<br>\n",
    "input layer has no learnable parameters because it has no inputs to its neurons\n",
    "\n",
    "\n",
    "# 20. Learnable Parameters for a Convolutional Neural Network\n",
    "Convolutional layers have filters that are also learnable parameters and must be accounted as so to this formula with some changes : (weights x outputs) + biases\n",
    "<br>\n",
    "For input:  if last layer is dense, then it is # of nodes; if last layer is conv, then it is # of filters\n",
    "<br>\n",
    "For outputs: # of filters x 2D size of filters\n",
    "<br>\n",
    "For biases: # of filters\n",
    "<br>\n",
    "We must flatten data when going from a Conv layer to a Dense layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21. Regularization\n",
    "Add a term to our loss function to penalize for large weights\n",
    "<br>\n",
    "This term is an equation including a regularization parameter (lambda) that is tweaked as it learns\n",
    "<br>\n",
    "If lambda is large, it could lower weights of some layers to reduce complexity since SGD wants to minimize the loss function\n",
    "<br>\n",
    "Increasing the regularization parameter would lower weights which lessens layer complexity and probably lessen overfitting\n",
    "<br>\n",
    "Most common is L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 1, 16)             96        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1, 32)             544       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1, 2)              66        \n",
      "=================================================================\n",
      "Total params: 706\n",
      "Trainable params: 706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#L2 regularization used below with regularization parameter lambda as value 0.01\n",
    "from keras import regularizers\n",
    "model = Sequential([\n",
    "    Dense(16, input_shape=(1,5), activation='relu'),\n",
    "    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 22. Batch Size\n",
    "Batch size is a group of data fed to the model, while epoch occurs when the whole data is finally fed\n",
    "<br>\n",
    "Larger batches = faster training, but our computer may not be able to handle too large batch sizes\n",
    "<br>\n",
    "Batch size is another hyperparameter we must test and tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size sets 10 data points at a time until we pass in all data to complete 1 epoch\n",
    "model.fit(\n",
    "    x=scaled_train_samples, \n",
    "    y=train_labels, \n",
    "    validation_data=valid_set, \n",
    "    batch_size=10,\n",
    "    epochs=20, \n",
    "    shuffle=True, \n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 23. Fine Tuning and Transfer Learning\n",
    "Tweaking a model to better a designed and trained model that has already learned\n",
    "<br>\n",
    "To fine tune, import the original model, then remove and add layers to fit your task's needs. Freeze the weights of the layers that you want to keep so they are not changed. Only update the weights of the new layers you added and need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 24. Batch Normalization\n",
    "Normalize or Standardization are used in the preprocessing process\n",
    "<br>\n",
    "Normalize: scaling down large data points\n",
    "<br>\n",
    "Standardize: Puts values to scale in normal distribution\n",
    "<br>\n",
    "Useful if features vary widely, which can cause imbalanced gradients\n",
    "<br>\n",
    "### Batch Norm\n",
    "Batch Norm directly normalizes output from activation function before being passed to next layer. Then multiplies and adds by arbitrary trainable parameters to be optimized. This helps with balancing weights in the network.\n",
    "<br>\n",
    "Happens on a per batch basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#beta_initializer: Initializer for beta weight\n",
    "#gamma_initializer: Initializer for gamma weight\n",
    "#axis is the axis u want normalized, axis=1 usually features\n",
    "from keras.layers import BatchNormalization\n",
    "model = Sequential([\n",
    "    Dense(16, input_shape=(1,5), activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    BatchNormalization(axis=1)           #set this after the layer for which u want the output normalized\n",
    "    Dense(2, activation='softmax')\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
